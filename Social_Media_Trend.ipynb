{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\src#36\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "15:26:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:26:59 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 20000 posts in 'Reddit_Posts_20k.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import requests.auth\n",
    "import pandas as pd\n",
    "import time\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from prophet import Prophet\n",
    "\n",
    "# Reddit API credentials\n",
    "Client_Id = \"Wkd1xI3snG5zIigFeGAIrg\"\n",
    "Secret_Key = \"75oRnclrvfEfPS-oMW2rMGoGRM9plw\"\n",
    "\n",
    "# Authentication\n",
    "Auth = requests.auth.HTTPBasicAuth(Client_Id, Secret_Key)\n",
    "\n",
    "# Data for token request\n",
    "Data = {\"grant_type\": \"client_credentials\"}\n",
    "\n",
    "# Headers\n",
    "Headers = {\n",
    "    \"User-Agent\": \"MyRedditApp/0.0.1\",\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "}\n",
    "\n",
    "# Request access token\n",
    "res = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=Auth, data=Data, headers=Headers)\n",
    "\n",
    "if res.status_code == 200:\n",
    "    TOKEN = res.json().get(\"access_token\")  # Get access token\n",
    "    Headers[\"Authorization\"] = f\"Bearer {TOKEN}\"  # Add Authorization header\n",
    "\n",
    "    all_posts = []  # Store all posts\n",
    "    total_fetched = 0  # Counter for posts\n",
    "    sort_methods = [\"hot\", \"top\", \"new\", \"rising\"]  # Fetch from multiple categories\n",
    "\n",
    "    while total_fetched < 20000:  # Loop until 20,000 posts are collected\n",
    "        for sort in sort_methods:\n",
    "            after = None  # Reset pagination for each sort method\n",
    "\n",
    "            while total_fetched < 20000:\n",
    "                response = requests.get(f\"https://oauth.reddit.com/r/Python/{sort}\", headers=Headers, params={\"limit\": 100, \"after\": after, \"t\": \"all\"})\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    posts = data[\"data\"][\"children\"]\n",
    "                    \n",
    "                    if not posts:\n",
    "                        break\n",
    "\n",
    "                    for post in posts:\n",
    "                        post_data = post[\"data\"]\n",
    "                        all_posts.append({\n",
    "                            \"title\": post_data.get(\"title\", \"N/A\"),\n",
    "                            \"ups\": post_data.get(\"ups\", 0),\n",
    "                            \"downs\": post_data.get(\"downs\", 0),\n",
    "                            \"score\": post_data.get(\"score\", 0),\n",
    "                            \"subreddit\": post_data.get(\"subreddit\", \"Unknown\"),\n",
    "                            \"selftext\": post_data.get(\"selftext\", \"\"),\n",
    "                            \"upvote_ratio\": post_data.get(\"upvote_ratio\", 0.0),\n",
    "                            \"post_hint\": post_data.get(\"post_hint\", \"N/A\"),\n",
    "                            \"num_comments\": post_data.get(\"num_comments\", 0),\n",
    "                            \"created_utc\": pd.to_datetime(post_data.get(\"created_utc\", None), unit='s'),\n",
    "                            \"subreddit_subscribers\": post_data.get(\"subreddit_subscribers\", 0),\n",
    "                            \"is_self\": post_data.get(\"is_self\", False),\n",
    "                            \"is_video\": post_data.get(\"is_video\", False),\n",
    "                            \"domain\": post_data.get(\"domain\", \"Unknown\"),\n",
    "                            \"permalink\": post_data.get(\"permalink\", \"N/A\"),\n",
    "                            \"num_crossposts\": post_data.get(\"num_crossposts\", 0),\n",
    "                            \"author\": post_data.get(\"author\", \"Unknown\"),\n",
    "                            \"author_premium\": post_data.get(\"author_premium\", False),\n",
    "                            \"author_flair_text\": post_data.get(\"author_flair_text\", \"N/A\"),\n",
    "                            \"media_only\": post_data.get(\"media_only\", False),\n",
    "                            \"over_18\": post_data.get(\"over_18\", False),\n",
    "                            \"is_crosspostable\": post_data.get(\"is_crosspostable\", False),\n",
    "                            \"link_flair_text\": post_data.get(\"link_flair_text\", \"N/A\"),\n",
    "                            \"all_awardings\": len(post_data.get(\"all_awardings\", [])),\n",
    "                            \"gildings\": post_data.get(\"gildings\", {}),\n",
    "                        })\n",
    "\n",
    "                        total_fetched += 1\n",
    "                        if total_fetched >= 20000:\n",
    "                            break  # Stop fetching once we reach 20,000 rows\n",
    "\n",
    "                    after = data[\"data\"].get(\"after\")\n",
    "                    \n",
    "                    if not after:\n",
    "                        break  # Stop if no more pages available\n",
    "                    \n",
    "                    time.sleep(1)  # Respect API rate limit\n",
    "                else:\n",
    "                    print(f\"Error fetching {sort} posts:\", response.status_code, response.text)\n",
    "                    break\n",
    "\n",
    "        if total_fetched >= 20000:\n",
    "            break  # Ensure we stop the outer loop once we reach 20,000\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_posts)\n",
    "    \n",
    "    # Sentiment Analysis\n",
    "    df['sentiment'] = df['title'].apply(lambda x: \"Positive\" if TextBlob(x).sentiment.polarity > 0 else (\"Negative\" if TextBlob(x).sentiment.polarity < 0 else \"Neutral\"))\n",
    "    \n",
    "    # Topic Modeling\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(df['title'])\n",
    "    lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "    topics = lda.fit_transform(X)\n",
    "    df['topic'] = topics.argmax(axis=1)\n",
    "    \n",
    "    # Trend Prediction using Prophet\n",
    "    trend_df = df[['created_utc', 'num_comments']].dropna()\n",
    "    trend_df = trend_df.rename(columns={'created_utc': 'ds', 'num_comments': 'y'})\n",
    "    model = Prophet()\n",
    "    model.fit(trend_df)\n",
    "    future = model.make_future_dataframe(periods=30)\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # Save DataFrame to CSV\n",
    "    csv_filename = \"Reddit_Posts_20k.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    print(f\"Successfully saved {len(df)} posts in '{csv_filename}'\")\n",
    "else:\n",
    "    print(\"Error:\", res.status_code, res.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 262.1/981.5 kB ? eta -:--:--\n",
      "     ------------------------------ ------- 786.4/981.5 kB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 981.5/981.5 kB 2.0 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\src#36\\appdata\\roaming\\python\\python313\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (pyproject.toml): started\n",
      "  Building wheel for langdetect (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993364 sha256=9b2cbcc9b5671c592461b0e4d10b41acae65e12d5f645f3536696ddfaa47ba72\n",
      "  Stored in directory: c:\\users\\src#36\\appdata\\local\\pip\\cache\\wheels\\eb\\87\\25\\2dddf1c94e1786054e25022ec5530bfed52bad86d882999c48\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data Cleaning Complete! Saved as 'Reddit_Cleaned.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from langdetect import detect, DetectorFactory\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Set random seed for consistent language detection\n",
    "DetectorFactory.seed = 42\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Reddit_Posts_20k.csv\")\n",
    "\n",
    "# 1️⃣ **Remove Duplicates**\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 2️⃣ **Detect and Remove Non-English Posts**\n",
    "def detect_lang(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "df['language'] = df['title'].apply(lambda x: detect_lang(str(x)) if pd.notnull(x) else \"unknown\")\n",
    "df = df[df['language'] == 'en']  # Keep only English posts\n",
    "\n",
    "# 3️⃣ **Handle Missing Values**\n",
    "df = df.fillna(\"Unknown\")  # Replace missing values with \"Unknown\"\n",
    "\n",
    "# 4️⃣ **Normalize Data**  \n",
    "# Extract hashtags from the title\n",
    "df['hashtags'] = df['title'].apply(lambda x: re.findall(r\"#\\w+\", str(x)))\n",
    "\n",
    "# Extract mentions from selftext\n",
    "df['mentions'] = df['selftext'].apply(lambda x: re.findall(r\"@\\w+\", str(x)))\n",
    "\n",
    "# Convert timestamps\n",
    "df['created_utc'] = pd.to_datetime(df['created_utc'], errors='coerce')\n",
    "\n",
    "# 5️⃣ **Sentiment Analysis**  \n",
    "df['sentiment'] = df['title'].apply(lambda x: \"Positive\" if TextBlob(x).sentiment.polarity > 0 else (\"Negative\" if TextBlob(x).sentiment.polarity < 0 else \"Neutral\"))\n",
    "\n",
    "# 6️⃣ **Topic Modeling** (Latent Dirichlet Allocation)\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['title'])\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "topics = lda.fit_transform(X)\n",
    "df['topic'] = topics.argmax(axis=1)\n",
    "\n",
    "# Save cleaned dataset\n",
    "df.to_csv(\"Reddit_Cleaned.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Data Cleaning Complete! Saved as 'Reddit_Cleaned.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8729664243682935\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      0.55      0.71       326\n",
      "     Neutral       0.84      0.98      0.91      1662\n",
      "    Positive       0.93      0.79      0.85       901\n",
      "\n",
      "    accuracy                           0.87      2889\n",
      "   macro avg       0.92      0.77      0.82      2889\n",
      "weighted avg       0.89      0.87      0.87      2889\n",
      "\n",
      "Predicted Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "# df = pd.read_csv('Reddit_Posts_20k.csv')\n",
    "df = pd.read_csv('Reddit_Cleaned.csv')\n",
    "\n",
    "# Drop rows with missing values in relevant columns\n",
    "df = df.dropna(subset=['title', 'selftext', 'sentiment'])\n",
    "\n",
    "# Combine title and selftext for better context\n",
    "df['text'] = df['title'] + ' ' + df['selftext']\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Vectorization (Convert text to numerical format)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "y = df['sentiment']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training (Naïve Bayes classifier)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Function to predict sentiment for new text\n",
    "def predict_sentiment(text):\n",
    "    text = clean_text(text)\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    prediction = model.predict(text_vectorized)\n",
    "    return prediction[0]\n",
    "\n",
    "# Example prediction\n",
    "example_text = \"This is the best post I've seen!\"\n",
    "print(\"Predicted Sentiment:\", predict_sentiment(example_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
